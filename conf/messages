# Messages

# Spark Core
spark.core.introduction=<div class='callout bg-gray'><h4>Introduction Apache Spark</h4><p>Apache Spark is a powerful open source processing engine built around speed, ease of use, and sophisticated analytics.It is a cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is a framework for performing general data analytics on distributed computing cluster like Hadoop. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application. It provides in memory computations for increase speed and data process over mapreduce.It runs on top of existing hadoop cluster and access hadoop data store (HDFS), can also process structured data in Hive and Streaming data from HDFS,Flume,Kafka,Twitter.</p></div>
spark.core.feature=<div class='callout bg-gray'><h4>Featutres of Spark</h4><p>Some of Spark's features which are really highlighting it in the Big Data world.<br /><b>1.Speed</b></p><p>Spark can be 100x faster than Hadoop for large scale data processing by exploiting in memory computing and other optimizations. Spark makes it possible by reducing number of read/write to disc. It stores this intermediate processing data in-memory. It uses the concept of an Resilient Distributed Dataset (RDD), which allows it to transparently store data on memory and persist it to disc only it’s needed.</p><p><b>2.Ease of Use</b></p><p>Spark has easy-to-use APIs for operating on large datasets. This includes a collection of over 100 operators for transforming data and familiar data frame APIs for manipulating semi-structured data. Spark lets you quickly write applications in Java, Scala, or Python. This helps developers to create and run their applications on their familiar programming languages and easy to build parallel apps.</p></div>
spark.core.initialize=<div class='callout bg-gray'><h4>Initializing</h4><p>Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application. The appName parameter is a name for your application to show on the cluster UI. master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode.</p></div><code lang='scala'>val conf = new SparkConf().setAppName("appName").setMaster("master")</br>new SparkContext(conf)</code>

# Spark Core RDD
spark.core.rdd.create=<div class='callout bg-gray'><h4>Create RDD</h4><p>There are two ways to create RDDs − parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system</p><p><b>Using parallelize() Method</b></p><p><code lang='scala'>val lines = sc.parallelize(List("pandas", "i like pandas"))</code></p></div>
spark.core.rdd.operations.transformation=<div class='callout bg-gray'><h4>Transformations</h4><p>Transformations are operations on RDDs that return a new RDD. Some basic common transformations fuctions supported by Spark.</p><p><b>map()</b></p></div><code lang='scala'>val input1 = sc.parallelize(List(1, 2, 3, 4,5,6,7,8,9))<br/>val result1 = input.map(x => x + 1) <br/> println("Mapping:" + result1.collect().mkString(","))</code><br><a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-Core#1transformations'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.core.rdd.operations.actions=<div class='callout bg-gray'><h4>Actions</h4><p>Actions are operations that return a result to the driver program or write it to storage Some basic common actions fuctions supported by Spark.</p></div><p><b>reduce()</b></p><code lang='scala'>val input = sc.parallelize(List(3, 2, 4, 6)) <br> val inputs= sc.parallelize(List(2, 4, 2, 3)) <br> val rUnion = input.union(inputs) <br> val resultReduce = rUnion.reduce((x, y) => x + y) <br>println("reduce:" + resultReduce + " ")</code><br><a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-Core#2actions'>More info<i class="fa fa-arrow-circle-right"></i></a>

#Spark Sql
spark.sql.initializing=<div class='callout bg-gray'><h4>Initializing</h4><p>Spark SQL is a component on top of Spark Core that introduces a new data abstraction called SchemaRDD, which provides support for structured and semi-structured data.Spark SQL is to execute SQL queries written using either a basic SQL syntax or HiveQL. Spark SQL can also be used to read data from an existing Hive installation.It provides a programming abstraction called DataFrame and can act as distributed SQL query engine.</p></div><code lang='scala'>val sc = SparkCommon.sparkContext <br> val sqlContext = new org.apache.spark.sql.SQLContext(sc)</code>
spark.sql.basic=<div class='callout bg-gray'><h4>Basic Query</h4><p>Spark SQL is a component on top of Spark Core that introduces a new data abstraction called SchemaRDD, which provides support for structured and semi-structured data.Spark SQL is to execute SQL queries written using either a basic SQL syntax or HiveQL. Spark SQL can also be used to read data from an existing Hive installation.It provides a programming abstraction called DataFrame and can act as distributed SQL query engine.</p></div><code lang='scala'>val sc = SparkCommon.sparkContext <br> val sqlContext = new org.apache.spark.sql.SQLContext(sc)</code>

#Spark Dataframes
spark.dataframes.create=<div class='callout bg-gray'><h4>Dataframe</h4><p>A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques. A DataFrame can be constructed from an array of different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. Here we are using JSON document named cars.json with the following content and generate a table based on the schema in the JSON document.</p></div><code lang='scala'>val df = sqlContext.read.json("src/main/resources/cars.json")</code>

#API
spark.dataframes.api.action=<div class='callout bg-gray'><h4>Action</h4><p>Action are operations (such as take, count, first, and so on) that return a value after running a computation on an DataFrame.</p></div>
spark.dataframes.api.basic=<div class='callout bg-gray'><h4>Basic</h4><p><b>printSchema : </b> If you want to see the Structure (Schema) of the DataFrame, then use the following command.<br><code lang='scala'>carDataFrame.printSchema()</code><br><b>toDF() : </b> Returns a new DataFrame with columns renamed. It can be quite convenient in conversion from a RDD of tuples into a DataFrame with meaningful names.<br><code lang='scala'>val car = sc.textFile("src/main/resources/fruits.txt").map(_.split(",")).map(f => Fruit(f(0).trim.toInt, f(1), f(2).trim.toInt)).toDF().show()</code><br><b>dtypes() : </b>Returns all column names and their data types as an array.<br><code lang='scala'>carDataFrame.dtypes.foreach(println)</code ><br><b>columns () : </b>Returns all column names as an array.<br><code lang='scala'>carDataFrame.columns.foreach(println)</code ><br><b>Cache() : </b> Cache() explicitly to store the data into memory. Or data stored in a distributed way in the memory by default.</div><code lang='scala'>val resultCache = carDataFrame.filter(carDataFrame("speed") > 300)<br>resultCache.cache().show()</code><br><a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#basic-dataframe-functions'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.api.operations=<div class='callout bg-gray'><h4>Operations</h4><p><b><br>sort() : </b> Returns a new DataFrame sorted by the given expressions.<br><code lang='scala'>carDataFrame.sort($"itemNo".desc).show()</code><br><br><b>orderBy() : </b> Returns a new DataFrame sorted by the specified column(s) .<br><code lang='scala'>carDataFrame.orderBy(desc("speed")).show()</code><br><br><b>groupBy() : </b>counting the number of cars who are of the same speed.<br><code lang='scala'>carDataFrame.groupBy("speed").count().show()</code ><br><br><b>na() : </b>Returns a DataFrameNaFunctions for working with missing data.<br><code lang='scala'>carDataFrame.na.drop().show()</code ><br><br><b>alias() : </b> Returns a new DataFrame with an alias set.Same as as.<code lang='scala'>carDataFrame.select(avg($"weight").alias("avg_weight")).show()</code></div><a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#data-frame-operations'>More info<i class="fa fa-arrow-circle-right"></i></a>

spark.dataframes.interoperating.overview=<div class='callout bg-gray'><h4>Interoperating with RDDs</h4><p> SparkSQL supports two different types methods for converting existing RDDs into DataFrames: <b><br>1. Inferring the Schema using Reflection: </b> <br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#1-inferring-the-schema-using-reflection'>More info<i class="fa fa-arrow-circle-right"></i></a><br><b>2. Programmatically Specifying the Schema: </b> <br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#2-programmatically-specifying-the-schema'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.interoperating.infering=<div class='callout bg-gray'><h4>Inferring the Schema using Reflection:</h4><p> <br> The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and they become the names of the columns. RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements..<br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#1-inferring-the-schema-using-reflection'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.interoperating.specifying=<div class='callout bg-gray'><h4>Programmatically Specifying the Schema:</h4><p> <br> Creating DataFrame is through programmatic interface that allows you to construct a schema and then apply it to an existing RDD. DataFrame can be created programmatically with three steps. We Create an RDD of Rows from an Original RDD. Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step first. Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext...<br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#2-programmatically-specifying-the-schema'>More info<i class="fa fa-arrow-circle-right"></i></a>

#Sources
spark.dataframes.sources.overview=<div class='callout bg-gray'><h4>Data Sources</h4><p> Spark SQL supports a number of structured data sources. These sources include Hive tables, JSON, and Parquet files.Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on as normal RDDs and can also be registered as a temporary table. Registering a DataFrame as a table allows you to run SQL queries over its data. <b><br>1. DataFrame Operations in JSON file: </b> <br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#dataframe-operations-in-json-file'>More info<i class="fa fa-arrow-circle-right"></i></a><br><b>2. DataFrame Operations in Text file: </b> <br> <a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#dataframe-operations-in-text-file'>More info<i class="fa fa-arrow-circle-right"></i></a> <b>3. DataFrame Operations in CSV file : </b> <br> <a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#dataframe-operations-in-csv-file-'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.sources.json=<div class='callout bg-gray'><h4>JSON Operations</h4><p> <br> Here we include some basic examples of structured data processing using DataFrames. As an example, the following creates a DataFrame based on the content of a JSON file. Read a JSON document named cars.json with the following content and generate a table based on the schema in the JSON document.<br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#dataframe-operations-in-json-file'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.sources.txt=<div class='callout bg-gray'><h4>TXT Operations</h4><p> <br> As an example, the following creates a DataFrame based on the content of a text file. Read a text document named employee.txt with the following content and generate a table based on the schema in the text document..<br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#dataframe-operations-in-text-file'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.sources.csv=<div class='callout bg-gray'><h4>CSV Operations</h4><p> <br> As an example, the following creates a DataFrame based on the content of a CSV file. Read a csv document named cars.csv with the following content and generate a table based on the schema in the csv document..<br> <a href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#dataframe-operations-in-csv-file-'>More info<i class="fa fa-arrow-circle-right"></i></a>

#Interoperating
spark.dataframes.interoperating.overview=<div class='callout bg-gray'><h4>Overview</h4><p> SparkSQL supports two different types methods for converting existing RDDs into DataFrames: <b><br>1. Inferring the Schema using Reflection: </b> <br> <a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#1-inferring-the-schema-using-reflection'><center><font size="4" color="green">More...</font></center></a><br><b>2. Programmatically Specifying the Schema: </b> <br> <a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#2-programmatically-specifying-the-schema'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.interoperating.infering=<div class='callout bg-gray'><h4>Inferring Schema</h4><p> <br> The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and they become the names of the columns. RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements..<br> <a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#1-inferring-the-schema-using-reflection'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.dataframes.interoperating.specifying=<div class='callout bg-gray'><h4>Specifying Schema</h4><p> <br> Creating DataFrame is through programmatic interface that allows you to construct a schema and then apply it to an existing RDD. DataFrame can be created programmatically with three steps. We Create an RDD of Rows from an Original RDD. Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step first. Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext...<br> <a class="small-box-footer" href='https://github.com/rklick-solutions/spark-tutorial/wiki/Spark-SQL#2-programmatically-specifying-the-schema'>More info<i class="fa fa-arrow-circle-right"></i></a>

#Spark Datasets
spark.datasets.create=<div class='callout bg-gray'><h4>Dataset</h4><p>Dataset is a new experimental interface that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).</p></div><code lang='scala'>val lines = sqlContext.read.text("src/main/resources/test_file.txt").as[String] <br> val words = lines<br>.flatMap(_.split(" "))<br>.filter(_ != "")<br>.groupBy(_.toLowerCase)<br>.count()<br>.show()</code>
spark.datasets.basic=

#Spark Streaming
spark.streaming.overview=<div class='callout bg-gray'><h4>Overview</h4><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.</p></div><a class="small-box-footer" href='http://spark.apache.org/docs/latest/streaming-programming-guide.html#overview'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.streaming.linking=<div class='callout bg-gray'><h4>Linking</h4><p>Similar to Spark, Spark Streaming is available through Maven Central. To write your own Spark Streaming program, you will have to add the following dependency to your SBT</p></div><code lang='scala'>libraryDependencies += "org.apache.spark" % "spark-streaming_2.10" % "1.6.0"</code><a class="small-box-footer" href='http://spark.apache.org/docs/latest/streaming-programming-guide.html#linking'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.streaming.initializing=<div class='callout bg-gray'><h4>Initializing</h4><p>To initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality</p><p>A StreamingContext object can be created from a SparkConf object.</p></div><code lang='scala'>import org.apache.spark._<br>import org.apache.spark.streaming._<br>val conf = new SparkConf().setAppName(appName).setMaster(master)<br>val ssc = new StreamingContext(conf, Seconds(1))<br>import org.apache.spark.streaming._<br>val conf = new SparkConf().setAppName(appName).setMaster(master)<br>val ssc = new StreamingContext(conf, Seconds(1))</code><br><a class="small-box-footer" href='http://spark.apache.org/docs/latest/streaming-programming-guide.html#initializing-streamingcontext'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.streaming.discretized=<div class='callout bg-gray'><h4>Discretized</h4><p>Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream</div><br><a class="small-box-footer" href='http://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams'>More info<i class="fa fa-arrow-circle-right"></i></a>

#Spark MLib
spark.mlib.overview.estimators=<div class='callout bg-gray'><h4>Estimators</h4><p>An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.</p></div><br><a class="small-box-footer" href='http://spark.apache.org/docs/latest/ml-guide.html#estimators'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.mlib.overview.transformers=<div class='callout bg-gray'><h4>Transformers</h4><p>A Transformer is an abstraction that includes feature transformers and learned models. Technically, a Transformer implements a method transform(), which converts one DataFrame into another, generally by appending one or more columns</p></div><br><a class="small-box-footer" href='http://spark.apache.org/docs/latest/ml-guide.html#transformers'>More info<i class="fa fa-arrow-circle-right"></i></a>
spark.mlib.overview.pipelines=<div class='callout bg-gray'><h4>Pipelines</h4><p>Transformer.transform()s and Estimator.fit()s are both stateless. In the future, stateful algorithms may be supported via alternative concepts.</p></div><br><a class="small-box-footer" href='http://spark.apache.org/docs/latest/ml-guide.html#properties-of-pipeline-components'>More info<i class="fa fa-arrow-circle-right"></i></a>

spark.mlib.algorithm.regression=
spark.mlib.algorithm.clustering=
spark.mlib.algorithm.classification=

#Spark GraphX
spark.graphx.overview=<div class='callout bg-gray'><h4>Overview</h4><p>GraphX is a new component in Spark for graphs and graph-parallel computation.</p><p></p></div><br><a class="small-box-footer" href="http://spark.apache.org/docs/1.6.0/graphx-programming-guide.html#overview">More info<i class="fa fa-arrow-circle-right"></i></a>
spark.graphx.algorithm.pagerank=<div class='callout bg-gray'><h4>Page Rank</h4><p>PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.</p></div><code lange='scala'>// Load the edges as a graph<br>val graph = GraphLoader.edgeListFile(sc, "graphx/data/followers.txt")<br> // Run PageRank<br>val ranks = graph.pageRank(0.0001).vertices</code><br><a class="small-box-footer" href="http://spark.apache.org/docs/1.6.0/graphx-programming-guide.html#pagerank">More info<i class="fa fa-arrow-circle-right"></i></a>
spark.graphx.algorithm.triangle=<div class='callout bg-gray'><h4>Triangle</h4><p>Triangle Count is very useful in social network analysis. The triangle is a three-node small graph, where every two nodes are connected.</p></div><code lange='scala'>// Load the edges in canonical order and partition the graph for triangle count<br>val graph = GraphLoader.edgeListFile(sc, "graphx/data/followers.txt", true).partitionBy(PartitionStrategy.RandomVertexCut)<br>// Find the triangle count for each vertex<br>val triCounts = graph.triangleCount().vertices</code><br><a class="small-box-footer" href="http://spark.apache.org/docs/1.6.0/graphx-programming-guide.html#triangle-counting">More info<i class="fa fa-arrow-circle-right"></i></a>
spark.graphx.algorithm.connected=<div class='callout bg-gray'><h4>Connected</h4><p>Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex.</p></div><code lange='scala'>// Load the graph<br>val graph = GraphLoader.edgeListFile(sc, "graphx/data/followers.txt", true)<br>// Find the connected components <br>val cc = graph.connectedComponents().vertices</code><br><a class="small-box-footer" href="http://spark.apache.org/docs/1.6.0/graphx-programming-guide.html#connected-components">More info<i class="fa fa-arrow-circle-right"></i></a>
spark.graphx.algorithm.strongly=<div class='callout bg-gray'><h4>Strongly</h4><p>A pair of vertices u and v are said to be strongly connected to each other if there is a path in each direction between them.</p></div><code lange='scala'>// Load the edges in canonical order <br>val graph = GraphLoader.edgeListFile(sc, "graphx/data/followers.txt", true)<br>//Find Strongly Connected Component<br>val stronglyConnected = graph.stronglyConnectedComponents(3).vertices</code><br><a class="small-box-footer" href="http://spark.apache.org/docs/1.6.0/graphx-programming-guide.html">More info<i class="fa fa-arrow-circle-right"></i></a>

#SparkR
spark.sparkr=