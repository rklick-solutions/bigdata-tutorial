
# Messages

welcome=Hello {0}!

# Spark
spark.dataframes=<div class='callout bg-gray'><h4>Creating DataFrames</h4><p>A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques. A DataFrame can be constructed from an array of different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. Here we are using JSON document named cars.json with the following content and generate a table based on the schema in the JSON document.</p></div><code lang='scala'>def overview = Action {Ok(views.html.spark_overview("Your new application is ready."))}</code><hr><img class="img-responsive" src='/assets/images/creatingdataframe.png' />
spark.datasets=<div class='callout bg-gray'><h4>Creating DataFrames</h4><p>A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques. A DataFrame can be constructed from an array of different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. Here we are using JSON document named cars.json with the following content and generate a table based on the schema in the JSON document.</p></div><code lang='scala'>def overview = Action {Ok(views.html.spark_overview("Your new application is ready."))}</code><hr><img class="img-responsive" src='/assets/images/creatingdataframe.png' />

# Spark Core
spark.core.intro=<div class='callout bg-gray'><h4>Introduction Apache Spark</h4><p>Apache Spark is a powerful open source processing engine built around speed, ease of use, and sophisticated analytics.It is a cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is a framework for performing general data analytics on distributed computing cluster like Hadoop. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application. It provides in memory computations for increase speed and data process over mapreduce.It runs on top of existing hadoop cluster and access hadoop data store (HDFS), can also process structured data in Hive and Streaming data from HDFS,Flume,Kafka,Twitter.</p></div>
spark.core.feature=<div class='callout bg-gray'><h4>Featutres of Spark</h4><p>Some of Spark's features which are really highlighting it in the Big Data world.<br /><b>1.Speed</b></p><p>Spark can be 100x faster than Hadoop for large scale data processing by exploiting in memory computing and other optimizations. Spark makes it possible by reducing number of read/write to disc. It stores this intermediate processing data in-memory. It uses the concept of an Resilient Distributed Dataset (RDD), which allows it to transparently store data on memory and persist it to disc only itâ€™s needed.</p><p><b>2.Ease of Use</b></p><p>Spark has easy-to-use APIs for operating on large datasets. This includes a collection of over 100 operators for transforming data and familiar data frame APIs for manipulating semi-structured data. Spark lets you quickly write applications in Java, Scala, or Python. This helps developers to create and run their applications on their familiar programming languages and easy to build parallel apps.</p></div>


